{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayotomiwaa/aml-numerai-2/blob/main/Numerai_Variance_Inflation_Factor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hP5khNLMaDHy"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q numerapi pandas pyarrow matplotlib lightgbm scikit-learn cloudpickle lazypredict pandas-profiling\n",
        "\n",
        "# Inline plots\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAJbuNX2cPZN"
      },
      "source": [
        "## 1. Dataset  \n",
        "\n",
        "At a high level, the Numerai dataset is a tabular dataset that describes the stock market over time.\n",
        "\n",
        "Each row represents a stock at a specific point in time, where `id` is the stock id and the `era` is the date. The `features` describe the attributes of the stock (eg. P/E ratio) known on the date and the `target` is a measure of 20-day returns.\n",
        "\n",
        "The unique thing about Numerai's dataset is that it is `obfuscated`, which means that the underlying stock ids, feature names, and target definitions are anonymized. This makes it so that we can give this data out for free and so that it can be modeled without any financial domain knowledge (or bias!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8koHeUevbvTN"
      },
      "source": [
        "### Downloading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8q8ZWJobzhN"
      },
      "outputs": [],
      "source": [
        "# Initialize NumerAPI - the official Python API client for Numerai\n",
        "from numerapi import NumerAPI\n",
        "napi = NumerAPI()\n",
        "\n",
        "# Print all files available for download in the latest dataset\n",
        "[f for f in napi.list_datasets() if f.startswith(\"v4.2\")]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chhfM1IGb2kF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# # Download the training data and feature metadata\n",
        "# # This will take a few minutes ðŸµ\n",
        "napi.download_dataset(\"v4.2/train_int8.parquet\");\n",
        "napi.download_dataset(\"v4.2/features.json\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pPNAVBDcA39"
      },
      "outputs": [],
      "source": [
        "# Load only the \"medium\" feature set to reduce memory usage and speedup model training (required for Colab free tier)\n",
        "# Use the \"all\" feature set to use all features\n",
        "feature_metadata = json.load(open(\"v4.2/features.json\"))\n",
        "feature_cols = feature_metadata[\"feature_sets\"][\"medium\"]\n",
        "train = pd.read_parquet(\"v4.2/train_int8.parquet\", columns=[\"era\"] + feature_cols + [\"target\"])\n",
        "\n",
        "# Downsample to every 4th era to reduce memory usage and speedup model training (suggested for Colab free tier)\n",
        "# Comment out the line below to use all the data\n",
        "train = train[train[\"era\"].isin(train[\"era\"].unique()[::100])]\n",
        "train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnAy2cu_cVDV"
      },
      "source": [
        "### Eras\n",
        "As mentioned above, each `era` corresponds to a different date. Each era is exactly 1 week apart.\n",
        "\n",
        "It is helpful to think about rows of stocks within the same `era` as a single example. You will notice that throughout this notebook and other examples, we often talk about things \"per era\". For example, the number of rows per era represents the number of stocks in Numerai's investable universe on that date."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opPp4kd9cZf1"
      },
      "outputs": [],
      "source": [
        "# Plot the number of rows per era\n",
        "train.groupby(\"era\").size().plot(title=\"Number of rows per era\", figsize=(5, 3), xlabel=\"Era\");"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Variance Inflation Factor"
      ],
      "metadata": {
        "id": "MprhMciVKRTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = train.drop(['era','target'],axis=1).iloc[: ,;5]\n",
        "# Convert int8 columns to float64\n",
        "for col in data.select_dtypes(include=['int8']).columns:\n",
        "    data[col] = data[col].astype('float64')"
      ],
      "metadata": {
        "id": "4pb5Lnj2KVGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Assuming 'data' is your Numerai dataset loaded as a pandas DataFrame\n",
        "# Replace 'data' with the name of your DataFrame\n",
        "\n",
        "# Select only the numerical features for VIF calculation\n",
        "numerical_features = data.select_dtypes(include=['float64', 'int64'])\n",
        "\n",
        "# Threshold for VIF\n",
        "vif_threshold = 5.0  # You can adjust this threshold as needed\n",
        "\n",
        "# DataFrames to store the final results\n",
        "remaining_columns = pd.DataFrame(columns=['feature', 'VIF'])\n",
        "removed_columns = pd.DataFrame(columns=['feature', 'VIF'])\n",
        "\n",
        "while True:\n",
        "    # Calculating VIF for each feature\n",
        "    vif_data = pd.DataFrame()\n",
        "    vif_data[\"feature\"] = numerical_features.columns\n",
        "    vif_data[\"VIF\"] = [variance_inflation_factor(numerical_features.values, i) for i in range(len(numerical_features.columns))]\n",
        "\n",
        "    # Find max VIF\n",
        "    max_vif = vif_data['VIF'].max()\n",
        "\n",
        "    if max_vif > vif_threshold:\n",
        "        # Find feature with max VIF\n",
        "        max_vif_feature = vif_data[vif_data['VIF'] == max_vif]['feature'].iloc[0]\n",
        "\n",
        "        # Add to removed columns\n",
        "        removed_columns = removed_columns.append({'feature': max_vif_feature, 'VIF': max_vif}, ignore_index=True)\n",
        "\n",
        "        # Drop the feature with max VIF\n",
        "        numerical_features.drop(columns=[max_vif_feature], inplace=True)\n",
        "    else:\n",
        "        # All VIFs are below the threshold\n",
        "        remaining_columns = vif_data\n",
        "        break\n",
        "\n",
        "print(\"Remaining Columns:\\n\", remaining_columns)\n",
        "print(\"\\nRemoved Columns:\\n\", removed_columns)"
      ],
      "metadata": {
        "id": "872ddknlKeY8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgRWkUcC/xQxnD0n76g3ij",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}